{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6f3e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/train.py\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "from .schema import TARGET_COL, CATEGORICAL_COLS, NUMERIC_COLS\n",
    "from .data_utils import load_bank_marketing_df\n",
    "\n",
    "# xgboost optional\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "    warnings.warn(\"xgboost not available; skipping XGBClassifier.\")\n",
    "\n",
    "\n",
    "def build_preprocessor() -> ColumnTransformer:\n",
    "    cat_pipe = Pipeline(steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "    ])\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale\", StandardScaler())\n",
    "    ])\n",
    "    pre = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", cat_pipe, CATEGORICAL_COLS),\n",
    "            (\"num\", num_pipe, NUMERIC_COLS),\n",
    "        ]\n",
    "    )\n",
    "    return pre\n",
    "\n",
    "\n",
    "def train_models(X_train, y_train) -> Dict[str, Pipeline]:\n",
    "    pre = build_preprocessor()\n",
    "\n",
    "    models: Dict[str, Pipeline] = {}\n",
    "\n",
    "    # 1) Logistic Regression (class_weight to address imbalance)\n",
    "    lr = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\"))\n",
    "    ])\n",
    "    lr.fit(X_train, y_train)\n",
    "    models[\"logreg\"] = lr\n",
    "\n",
    "    # 2) Random Forest\n",
    "    rf = Pipeline([\n",
    "        (\"pre\", pre),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, n_jobs=-1, random_state=42,\n",
    "            class_weight=\"balanced\"\n",
    "        ))\n",
    "    ])\n",
    "    rf.fit(X_train, y_train)\n",
    "    models[\"rf\"] = rf\n",
    "\n",
    "    # 3) XGBoost (optional)\n",
    "    if HAS_XGB:\n",
    "        xgb = Pipeline([\n",
    "            (\"pre\", pre),\n",
    "            (\"clf\", XGBClassifier(\n",
    "                n_estimators=400,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.05,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                reg_lambda=1.0,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                tree_method=\"hist\",\n",
    "                eval_metric=\"logloss\"\n",
    "            ))\n",
    "        ])\n",
    "        xgb.fit(X_train, y_train)\n",
    "        models[\"xgb\"] = xgb\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def evaluate(model: Pipeline, X, y) -> Dict[str, Any]:\n",
    "    y_pred = model.predict(X)\n",
    "    # target labels: \"yes\" / \"no\"\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y, y_pred)),\n",
    "        \"precision_yes\": float(precision_score(y, y_pred, pos_label=\"yes\")),\n",
    "        \"recall_yes\": float(recall_score(y, y_pred, pos_label=\"yes\")),\n",
    "        \"f1_yes\": float(f1_score(y, y_pred, pos_label=\"yes\")),\n",
    "        \"report\": classification_report(y, y_pred, output_dict=False)\n",
    "    }\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\">>> Loading dataset from UCI...\")\n",
    "    df = load_bank_marketing_df()\n",
    "    print(\"Shape:\", df.shape)\n",
    "\n",
    "    # Ensure columns exist\n",
    "    missing = set(CATEGORICAL_COLS + NUMERIC_COLS + [TARGET_COL]) - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing expected columns: {missing}\")\n",
    "\n",
    "    # Split 70/15/15\n",
    "    X = df[CATEGORICAL_COLS + NUMERIC_COLS].copy()\n",
    "    y = df[TARGET_COL].astype(str)  # \"yes\"/\"no\"\n",
    "\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.15, random_state=42, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.1765, random_state=42, stratify=y_train_full\n",
    "    )\n",
    "    # 0.1765 of 0.85 â‰ˆ 0.15 overall, so 70/15/15\n",
    "\n",
    "    print(f\"Splits -> train: {X_train.shape}, val: {X_val.shape}, test: {X_test.shape}\")\n",
    "\n",
    "    print(\">>> Training models...\")\n",
    "    models = train_models(X_train, y_train)\n",
    "\n",
    "    print(\">>> Evaluating on validation set...\")\n",
    "    val_scores = {name: evaluate(m, X_val, y_val) for name, m in models.items()}\n",
    "    for name, sc in val_scores.items():\n",
    "        print(f\"\\nModel: {name}\")\n",
    "        print(f\"F1(yes)={sc['f1_yes']:.4f}  Acc={sc['accuracy']:.4f}  \"\n",
    "              f\"P={sc['precision_yes']:.4f}  R={sc['recall_yes']:.4f}\")\n",
    "        print(sc[\"report\"])\n",
    "\n",
    "    # Select best by F1 on positive class ('yes')\n",
    "    best_name = max(val_scores, key=lambda k: val_scores[k][\"f1_yes\"])\n",
    "    best_model = models[best_name]\n",
    "    print(f\"\\n>>> Best on validation: {best_name}\")\n",
    "\n",
    "    print(\">>> Final evaluation on test set...\")\n",
    "    test_metrics = evaluate(best_model, X_test, y_test)\n",
    "    print(f\"Test F1(yes)={test_metrics['f1_yes']:.4f}  Acc={test_metrics['accuracy']:.4f}\")\n",
    "    print(test_metrics[\"report\"])\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    dump(best_model, \"models/model.pkl\")\n",
    "    with open(\"models/metrics.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"chosen_model\": best_name,\n",
    "            \"val_metrics\": val_scores[best_name],\n",
    "            \"test_metrics\": test_metrics\n",
    "        }, f, indent=2)\n",
    "\n",
    "    print(\">>> Saved best model to models/model.pkl\")\n",
    "    print(\">>> Metrics snapshot to models/metrics.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
